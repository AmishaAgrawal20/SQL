{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\programdata\\anaconda\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: py4j==0.10.9 in c:\\programdata\\anaconda\\lib\\site-packages (from pyspark) (0.10.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum,count\n",
    "from pyspark.sql.functions import stddev_pop\n",
    "from pyspark.sql.functions import max\n",
    "from pyspark.sql.functions import mean ,corr,max, min,stddev_pop ,stddev_samp,covar_pop,covar_samp\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc =SparkContext.getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading textfile and FLATMAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['web', 'and', 'data', 'science', 'mathematical', 'modelling']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rdd = sc.textFile(\"sample.txt\")\n",
    "my_rdd = my_rdd.flatMap(lambda r: r.split(\" \"))\n",
    "my_rdd.collect()\n",
    "#map_result = data.flatmap(lambda r: r[5].split(\"|\"))\n",
    "#map_result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['web', 'and', 'data', 'science'], ['mathematical', 'modelling']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = sc.textFile(\"sample.txt\")\n",
    "map_result1 = data1.map(lambda line:line.split(\" \"))\n",
    "map_result1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new word or letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BIG DATA', 'course'),\n",
       " ('DATA SCIENCE', 'course'),\n",
       " ('OPTIMIZATION', 'course'),\n",
       " ('NETWORK THEORY', 'course')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sc.parallelize(\n",
    "        [\"BIG DATA\",\n",
    "         \"DATA SCIENCE\",\n",
    "        \"OPTIMIZATION\",\n",
    "        \"NETWORK THEORY\"]\n",
    "        )\n",
    "\n",
    "data2= words.map(lambda r: (r,(\"course\")))\n",
    "data2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BIG DATA', 'DATA SCIENCE']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sc.parallelize(\n",
    "        [\"BIG DATA\",\n",
    "         \"DATA SCIENCE\",\n",
    "        \"OPTIMIZATION\",\n",
    "        \"NETWORK THEORY\"]\n",
    "        )\n",
    "data3 = words.filter(lambda r:\"DATA\" in r)\n",
    "data3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.option(\"inferschema\",\"true\").option(\"header\",\"true\").csv(\"assignment01_flight.csv\")\n",
    "df.show()\n",
    "#print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINDING COUNTS WITH GREATER THAN CONDITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     208|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"flighttable\")\n",
    "spark.sql(\"\"\"select count(*) from flighttable where count>'10'\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df.where(col(\"count\") > '10')\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINDING COUNT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|    411352|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select SUM(count) from flighttable where DEST_COUNTRY_NAME = 'United States'\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|    411352|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.where(col(\"DEST_COUNTRY_NAME\") == \"United States\").agg(sum(\"count\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+\n",
      "|DEST_COUNTRY_NAME|sum(count)|\n",
      "+-----------------+----------+\n",
      "|    United States|    411352|\n",
      "+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7=df.select((\"*\")).filter(col(\"DEST_COUNTRY_NAME\") == \"United States\").groupBy(\"DEST_COUNTRY_NAME\").sum()\n",
    "df7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Sort [ORIGIN_COUNTRY_NAME#17 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(ORIGIN_COUNTRY_NAME#17 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#243]\n",
      "   +- *(2) HashAggregate(keys=[ORIGIN_COUNTRY_NAME#17], functions=[sum(cast(count#18 as bigint))])\n",
      "      +- Exchange hashpartitioning(ORIGIN_COUNTRY_NAME#17, 200), ENSURE_REQUIREMENTS, [id=#239]\n",
      "         +- *(1) HashAggregate(keys=[ORIGIN_COUNTRY_NAME#17], functions=[partial_sum(cast(count#18 as bigint))])\n",
      "            +- *(1) Project [ORIGIN_COUNTRY_NAME#17, count#18]\n",
      "               +- *(1) Filter (isnotnull(DEST_COUNTRY_NAME#16) AND (DEST_COUNTRY_NAME#16 = United States))\n",
      "                  +- FileScan csv [DEST_COUNTRY_NAME#16,ORIGIN_COUNTRY_NAME#17,count#18] Batched: false, DataFilters: [isnotnull(DEST_COUNTRY_NAME#16), (DEST_COUNTRY_NAME#16 = United States)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/User/assignment01_flight.csv], PartitionFilters: [], PushedFilters: [IsNotNull(DEST_COUNTRY_NAME), EqualTo(DEST_COUNTRY_NAME,United States)], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7=df.filter(col(\"DEST_COUNTRY_NAME\") == \"United States\").groupBy(\"ORIGIN_COUNTRY_NAME\").agg(sum(\"count\")).sort(col(\"ORIGIN_COUNTRY_NAME\"))\n",
    "df7.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINDING MAX VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|    370002|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select max(count) from flighttable \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|    370002|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df.select(max(\"count\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
